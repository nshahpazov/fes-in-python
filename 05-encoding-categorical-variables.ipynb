{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Encoding Categorical Variables**\n",
    "\n",
    "The most basic approach to representing categorical values as numeric data is to create dummy or indicator variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When having a rare categorical variable, it might not appear in the resampling process. A zero-variance filter might help in that case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to resampling we might try to establish which variables are near zero variance ones and remove them from the pool.\n",
    "A <b style=\"color: #ff4299\">threshold</b> can be used to determine when a variable is near-zero variance.\n",
    "\n",
    "Even though, we might not want to remove those variables. Another approach is to include a category <u style=\"color: #ff4299\">Other</u> that pools the rarer categories.\n",
    "\n",
    "Again, a <b style=\"color: #ff4299\">threshold</b> can be used to determine when a variable is the rare case. Since this step might affect the metrics of the model, we include it in the resampling process as well.\n",
    "\n",
    "Another approach is hashing. When using hashes to create dummy variables, the procedure is called **“feature hashing**” or the **“hash trick**” "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'location' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/m5/zsd1m_dd4wd36c3_4bftzh280000gn/T/ipykernel_53802/188974248.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeatureHasher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'dog'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cat'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'elephant'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'dog'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'run'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_towns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'location' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "h = FeatureHasher(n_features=16, dtype=bool)\n",
    "D = [{'dog': 1, 'cat': 2, 'elephant': 4}, {'dog': 2, 'run': 5}]\n",
    "f = h.transform(location[location.isin(sample_towns)])\n",
    "f.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "okc_train_df = pd.read_csv(\"./data/okc/okc_train.csv\")\n",
    "okc_test_df = pd.read_csv(\"./data/okc/okc_test.csv\")\n",
    "okc_down_df = pd.read_csv(\"./data/okc/okc_down.csv\")\n",
    "okc_sampled_df = pd.read_csv(\"./data/okc/okc_sampled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>where_town</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>77</td>\n",
       "      <td>alameda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>625</td>\n",
       "      <td>albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50</td>\n",
       "      <td>belmont</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index where_town\n",
       "0     77    alameda\n",
       "1    625     albany\n",
       "2     50    belmont"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "sample_towns = [\n",
    "  'alameda', 'belmont', 'benicia', 'berkeley', 'castro_valley', 'daly_city', \n",
    "  'emeryville', 'fairfax', 'martinez', 'menlo_park', 'mountain_view', 'oakland', \n",
    "  'other', 'palo_alto', 'san_francisco', 'san_leandro', 'san_mateo', \n",
    "  'san_rafael', 'south_san_francisco', 'walnut_creek'\n",
    "]\n",
    "\n",
    "location = okc_train_df['where_town'].drop_duplicates().sort_values().reset_index()\n",
    "location.head(3)\n",
    "\n",
    "# feature_hasher = FeatureHasher(n_features=16, alternate_sign=False)\n",
    "\n",
    "# feature_hasher.fit_transform(location['where_town'])\n",
    "# location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51, 2)\n",
      "               location    0    1    2    3    4    5    6    7    8    9  \\\n",
      "0               alameda  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "2               belmont  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "4               benicia  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "5              berkeley  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0   \n",
      "7         castro_valley  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0   \n",
      "9             daly_city  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0   \n",
      "12           emeryville  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "13              fairfax  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0   \n",
      "21             martinez  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0   \n",
      "22           menlo_park  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "26        mountain_view  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "28              oakland  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0   \n",
      "30                other  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0   \n",
      "32            palo_alto  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0   \n",
      "40        san_francisco  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0   \n",
      "41          san_leandro  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "43            san_mateo  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0   \n",
      "45           san_rafael  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0   \n",
      "47  south_san_francisco  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0   \n",
      "50         walnut_creek  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "\n",
      "     10   11   12   13   14   15  \n",
      "0   0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2   0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4   0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "5   0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "7   0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "9   0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "12  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "13  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "21  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "22  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "26  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "28  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "30  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "32  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "40  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "41  0.0  0.0  0.0  0.0  0.0  1.0  \n",
      "43  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "45  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "47  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "50  0.0  0.0  0.0  0.0  0.0  1.0  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer, CountVectorizer\n",
    "\n",
    "vectorizer = HashingVectorizer(n_features=2**4, binary=True, alternate_sign=False)\n",
    "X = vectorizer.fit_transform(location['where_town'])\n",
    "result_df = pd.DataFrame(X.toarray())\n",
    "\n",
    "# result_df['location'] = location['where_town']\n",
    "\n",
    "result_df.insert(0, 'location', location['where_town'])\n",
    "\n",
    "result_df = result_df[result_df['location'].isin(sample_towns)]\n",
    "\n",
    "print(location.shape)\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Less aliasing allows for better interpretation of the results. If a predictor has a significant impact on the model, it is probably a good idea understand why. When multiple categories are aliased due to a collision, untangling the true effect may be very difficult. Although this may help narrow down which categories are affecting the response.\n",
    "\n",
    "### Supervised Encoding\n",
    "\n",
    "#### **Likelihood encoding**\n",
    "\n",
    "For example, for the Ames housing data, we might calculate the mean or median sale price of a house for each neighborhood from the training data and use this statistic to represent the factor level in the model. There are a variety of ways to estimate the effects and these will be discussed below."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "48d3ba6fd8913976b5b8240356e2a25bc7de293def346b194c53738ff49bc619"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit ('3.9.2': pyenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
